ka_results['dataset'] = ds['dataset']
ka_results['type'] = ds['type']
results.append(ka_results)
output_file = '", output_dir, "/ka_results_combined.csv'
pd.concat(results).to_csv(output_file, index=False)
"
)
# Execute Python script
reticulate::py_run_string(python_script)
# Step 4: Read back results and plot
ka_results <- read.csv(file.path(output_dir, "ka_results_combined.csv"))
file_paths_orig <- tibble::tribble(
~file, ~dataset, ~type,
"data/annotated/manifestos_within_expanded.csv", "manifestos", "Original",
"data/annotated/manifestos_multi_within_expanded.csv", "manifestos_multi", "Original",
"data/annotated/mii_within_expanded.csv", "mii", "Original",
"data/annotated/mii_long_within_expanded.csv", "mii_long", "Original",
"data/annotated/news_within_expanded.csv", "news", "Original",
"data/annotated/news_short_within_expanded.csv", "news_short", "Original",
"data/annotated/stance_within_expanded.csv", "stance", "Original",
"data/annotated/stance_long_within_expanded.csv", "stance_long", "Original",
"data/annotated/synth_within_expanded.csv", "synth", "Original",
"data/annotated/synth_short_within_expanded.csv", "synth_short", "Original",
"data/annotated/tweets_pop_within_expanded.csv", "tweets_pop", "Original",
"data/annotated/tweets_rd_within_expanded.csv", "tweets_rd", "Original"
)
original_results <- file_paths_orig %>%
rowwise() %>%
mutate(original_data = list(read.csv(file) %>% select(iteration, ka_mean))) %>%
unnest(original_data) %>%
distinct(dataset, type, iteration, ka_mean)
combined_results <- bind_rows(
original_results,
ka_results
)
# Calculate ordering for facets
facet_order <- ka_results %>%
group_by(dataset) %>%
summarise(mean_pss = mean(ka_mean, na.rm = TRUE)) %>%
arrange(desc(mean_pss)) %>%
pull(dataset)
# Facet label mapping
facet_labels <- c(
"tweets_rd" = "Tweets (Rep. Dem.)",
"tweets_pop" = "Tweets (Populism)",
"news" = "News",
"news_short" = "News (Short)",
"manifestos" = "Manifestos",
"manifestos_multi" = "Manifestos Multi",
"stance" = "Stance",
"stance_long" = "Stance (Long)",
"mii" = "MII",
"mii_long" = "MII (Long)",
"synth" = "Synthetic",
"synth_short" = "Synthetic (Short)"
)
combined_results <- combined_results %>%
mutate(dataset = factor(dataset, levels = facet_order))
final_plot <- ggplot(combined_results, aes(x = iteration, y = ka_mean, color = type)) +
geom_line(alpha = .8) +
geom_point(alpha = .1) +
labs(
title = "",
x = "iteration",
y = "intra-PSS",
color = "Dataset"
) +
ylim(.75, 1) +
facet_wrap(
~ dataset,
scales = "free",
ncol = 4,
labeller = as_labeller(facet_labels)
) +
theme_minimal() +
theme(
legend.position = "bottom",
strip.text = element_text(size = 10)
)
print(final_plot)
ggsave("plots/combined_within_postpro.png", plot = final_plot, width = 8, height = 6, dpi = 300)
library(dplyr)
library(ggplot2)
library(reticulate)
library(tidyr)
# Set the Python executable for the virtual environment
use_python("pssenv/bin/python", required = TRUE)
# Define the output directory (same as in filtering)
output_dir <- "data/annotated/reannotated/between"
if (!dir.exists(output_dir)) {
dir.create(output_dir, recursive = TRUE)
}
# Input: File paths tibble referencing the filtered CSVs from the filtering script
file_paths <- tibble::tribble(
~file, ~dataset, ~type,
"data/annotated/reannotated/between/manifestos_multi_filtered.csv", "manifestos_multi", "Filtered",
"data/annotated/reannotated/between/manifestos_filtered.csv", "manifestos", "Filtered",
"data/annotated/reannotated/between/mii_long_filtered.csv", "mii_long", "Filtered",
"data/annotated/reannotated/between/mii_filtered.csv", "mii", "Filtered",
"data/annotated/reannotated/between/news_short_filtered.csv", "news_short", "Filtered",
"data/annotated/reannotated/between/news_filtered.csv", "news", "Filtered",
"data/annotated/reannotated/between/stance_long_filtered.csv", "stance_long", "Filtered",
"data/annotated/reannotated/between/stance_filtered.csv", "stance", "Filtered",
"data/annotated/reannotated/between/synth_short_filtered.csv", "synth_short", "Filtered",
"data/annotated/reannotated/between/synth_filtered.csv", "synth", "Filtered",
"data/annotated/reannotated/between/tweets_pop_filtered.csv", "tweets_pop", "Filtered",
"data/annotated/reannotated/between/tweets_rd_filtered.csv", "tweets_rd", "Filtered",
"data/annotated/reannotated/between/manifestos_multi_filtered_balanced.csv", "manifestos_multi", "Filtered & Balanced",
"data/annotated/reannotated/between/manifestos_filtered_balanced.csv", "manifestos", "Filtered & Balanced",
"data/annotated/reannotated/between/mii_long_filtered_balanced.csv", "mii_long", "Filtered & Balanced",
"data/annotated/reannotated/between/mii_filtered_balanced.csv", "mii", "Filtered & Balanced",
"data/annotated/reannotated/between/news_short_filtered_balanced.csv", "news_short", "Filtered & Balanced",
"data/annotated/reannotated/between/news_filtered_balanced.csv", "news", "Filtered & Balanced",
"data/annotated/reannotated/between/stance_long_filtered_balanced.csv", "stance_long", "Filtered & Balanced",
"data/annotated/reannotated/between/stance_filtered_balanced.csv", "stance", "Filtered & Balanced",
"data/annotated/reannotated/between/synth_short_filtered_balanced.csv", "synth_short", "Filtered & Balanced",
"data/annotated/reannotated/between/synth_filtered_balanced.csv", "synth", "Filtered & Balanced",
"data/annotated/reannotated/between/tweets_pop_filtered_balanced.csv", "tweets_pop", "Filtered & Balanced",
"data/annotated/reannotated/between/tweets_rd_filtered_balanced.csv", "tweets_rd", "Filtered & Balanced"
)
# Step 2: Write Python script (the script loads the CSVs, calculates KA, and writes combined results)
python_script <- "
import pandas as pd
from simpledorff import calculate_krippendorffs_alpha_for_df, metrics
def calculate_ka(df, dataset, annotator_col='prompt_id', class_col='annotation'):
df[annotator_col] = pd.to_numeric(df[annotator_col], errors='coerce')
df[class_col] = pd.to_numeric(df[class_col], errors='coerce')
df = df.dropna(subset=[annotator_col, class_col])
grouped = df.groupby('temperature')
results = []
# Use interval metric for manifestos_multi; nominal for others
metric_fn = metrics.interval_metric if dataset == 'manifestos_multi' else metrics.nominal_metric
for temp, group in grouped:
try:
alpha = calculate_krippendorffs_alpha_for_df(
group,
experiment_col='id',
annotator_col=annotator_col,
class_col=class_col,
metric_fn=metric_fn
)
results.append({'temperature': temp, 'ka_mean': alpha})
except Exception as e:
print(f'Error calculating KA for temperature {temp}: {e}')
return pd.DataFrame(results)
datasets = ["
# Append dataset entries (using the cleaned CSV files from filtering)
file_paths %>%
mutate(
dataset_entry = paste0(
"{'file': '", file, "', 'type': '", type, "', 'dataset': '", dataset, "'}"
)
) %>%
pull(dataset_entry) %>%
paste(collapse = ",\n") %>%
{python_script <<- paste0(python_script, ., "\n]")}
python_script <- paste0(
python_script,
"
results = []
for ds in datasets:
data = pd.read_csv(ds['file'])
ka_results = calculate_ka(data, dataset=ds['dataset'])
ka_results['dataset'] = ds['dataset']
ka_results['type'] = ds['type']
results.append(ka_results)
output_file = '", output_dir, "/ka_results_combined.csv'
pd.concat(results).to_csv(output_file, index=False)
"
)
# Execute Python script
reticulate::py_run_string(python_script)
# Step 4: Read back results and plot
ka_results <- read.csv(file.path(output_dir, "ka_results_combined.csv"))
# (The rest of your plotting code remains unchanged)
# Input: File paths for the original (uncleaned) files, if you wish to extract original KA values
file_paths_orig <- tibble::tribble(
~file, ~dataset, ~type,
"data/annotated/manifestos_between_expanded.csv", "manifestos", "Original",
"data/annotated/manifestos_multi_between_expanded.csv", "manifestos_multi", "Original",
"data/annotated/mii_between_expanded.csv", "mii", "Original",
"data/annotated/mii_long_between_expanded.csv", "mii_long", "Original",
"data/annotated/news_between_expanded.csv", "news", "Original",
"data/annotated/news_short_between_expanded.csv", "news_short", "Original",
"data/annotated/stance_between_expanded.csv", "stance", "Original",
"data/annotated/stance_long_between_expanded.csv", "stance_long", "Original",
"data/annotated/synth_between_expanded.csv", "synth", "Original",
"data/annotated/synth_short_between_expanded.csv", "synth_short", "Original",
"data/annotated/tweets_pop_between_expanded.csv", "tweets_pop", "Original",
"data/annotated/tweets_rd_between_expanded.csv", "tweets_rd", "Original"
)
original_results <- file_paths_orig %>%
rowwise() %>%
mutate(original_data = list(read.csv(file) %>% select(temperature, ka_mean))) %>%
unnest(original_data) %>%
distinct(dataset, type, temperature, ka_mean)
combined_results <- bind_rows(
original_results,
ka_results
)
# Calculate ordering for facets
facet_order <- ka_results %>%
group_by(dataset) %>%
summarise(mean_pss = mean(ka_mean, na.rm = TRUE)) %>%
arrange(desc(mean_pss)) %>%
pull(dataset)
# Facet label mapping
facet_labels <- c(
"tweets_rd" = "Tweets (Rep. Dem.)",
"tweets_pop" = "Tweets (Populism)",
"news" = "News",
"news_short" = "News (Short)",
"manifestos" = "Manifestos",
"manifestos_multi" = "Manifestos Multi",
"stance" = "Stance",
"stance_long" = "Stance (Long)",
"mii" = "MII",
"mii_long" = "MII (Long)",
"synth" = "Synthetic",
"synth_short" = "Synthetic (Short)"
)
combined_results <- combined_results %>%
mutate(dataset = factor(dataset, levels = facet_order))
final_plot <- ggplot(combined_results, aes(x = temperature, y = ka_mean, color = type)) +
geom_line(alpha = .8) +
geom_point(alpha = .1) +
labs(
title = "",
x = "Temperature",
y = "inter-PSS",
color = "Dataset"
) +
ylim(0, 1) +
facet_wrap(
~ dataset,
scales = "free",
ncol = 4,
labeller = as_labeller(facet_labels)
) +
theme_minimal() +
theme(
legend.position = "bottom",
strip.text = element_text(size = 10)
)
print(final_plot)
ggsave("plots/combined_between_postpro.png", plot = final_plot, width = 8, height = 6, dpi = 300)
library(dplyr)
library(ggplot2)
library(reticulate)
library(tidyr)
# Set the Python executable for the virtual environment
use_python("pssenv/bin/python", required = TRUE)
# --- Step 1: Define the file paths ---
file_paths <- tibble::tribble(
~file,                                    ~dataset,       ~type,
"data/annotated/reannotated/between/manifestos_multi_filtered.csv", "manifestos_multi", "Filtered",
"data/annotated/reannotated/between/manifestos_filtered.csv", "manifestos", "Filtered",
"data/annotated/reannotated/between/mii_long_filtered.csv", "mii_long", "Filtered",
"data/annotated/reannotated/between/mii_filtered.csv", "mii", "Filtered",
"data/annotated/reannotated/between/news_short_filtered.csv", "news_short", "Filtered",
"data/annotated/reannotated/between/news_filtered.csv", "news", "Filtered",
"data/annotated/reannotated/between/stance_long_filtered.csv", "stance_long", "Filtered",
"data/annotated/reannotated/between/stance_filtered.csv", "stance", "Filtered",
"data/annotated/reannotated/between/synth_short_filtered.csv", "synth_short", "Filtered",
"data/annotated/reannotated/between/synth_filtered.csv", "synth", "Filtered",
"data/annotated/reannotated/between/tweets_pop_filtered.csv", "tweets_pop", "Filtered",
"data/annotated/reannotated/between/tweets_rd_filtered.csv", "tweets_rd", "Filtered"
)
# --- Step 2: Define the output directory and file ---
output_dir <- "data/annotated/reannotated/comparison/"
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)
output_file <- file.path(output_dir, "ka_results_combined_subsamples.csv")
# --- Step 3: Build the Python script with injected file paths ---
# Create a string representing the Python list of dataset dictionaries
dataset_entries <- file_paths %>%
mutate(entry = paste0("{'file': '", file, "', 'type': '",
type, "', 'dataset': '", dataset, "'}")) %>%
pull(entry) %>%
paste(collapse = ",\n    ")
dataset_list_str <- paste0("datasets = [\n    ", dataset_entries, "\n]")
python_script <- paste0(
"import pandas as pd
from simpledorff import calculate_krippendorffs_alpha_for_df, metrics
def calculate_ka(df, dataset, annotator_col='prompt_id', class_col='annotation'):
# Convert columns to numeric and drop missing values
df[annotator_col] = pd.to_numeric(df[annotator_col], errors='coerce')
df[class_col] = pd.to_numeric(df[class_col], errors='coerce')
df = df.dropna(subset=[annotator_col, class_col])
# Group by 'temperature'
grouped = df.groupby('temperature')
results = []
# Choose the metric: interval for manifestos_multi, nominal for others
metric_fn = metrics.interval_metric if dataset == 'manifestos_multi' else metrics.nominal_metric
# Define subsample fractions
subsample_fracs = [0.02, 0.05, 0.1, 0.25, 0.5, 0.75]
for temp, group in grouped:
for frac in subsample_fracs:
try:
if len(group) > 0:
sample = group.sample(frac=frac, random_state=42)
alpha = calculate_krippendorffs_alpha_for_df(
sample,
experiment_col='id',
annotator_col=annotator_col,
class_col=class_col,
metric_fn=metric_fn
)
results.append({'temperature': temp, 'ka_mean': alpha, 'frac': frac})
except Exception as e:
print(f'Error calculating KA for temperature {temp} with frac {frac}: {e}')
return pd.DataFrame(results)
",
# Inject the dataset list constructed from R:
dataset_list_str, "\n\n",
"results = []
for ds in datasets:
data = pd.read_csv(ds['file'])
ka_results = calculate_ka(data, dataset=ds['dataset'])
ka_results['dataset'] = ds['dataset']
ka_results['type'] = ds['type']
results.append(ka_results)
output_file = '", output_file, "'
pd.concat(results).to_csv(output_file, index=False)
"
)
# --- Step 4: Run the Python script ---
reticulate::py_run_string(python_script)
# --- Step 5: Read back the results ---
ka_results <- read.csv(file.path(output_dir, "ka_results_combined_subsamples.csv"))
# Ensure 'frac' and 'temperature' are factors for plotting
ka_results$frac <- as.factor(ka_results$frac)
ka_results$temperature <- as.numeric(ka_results$temperature)
# --- Step 6: Order facets and rename them ---
# Calculate the mean KA per dataset for ordering (highest mean first)
facet_order <- ka_results %>%
group_by(dataset) %>%
summarise(mean_ka = mean(ka_mean, na.rm = TRUE)) %>%
arrange(desc(mean_ka)) %>%
pull(dataset)
# Define a named vector with new facet labels
facet_labels <- c(
"tweets_rd" = "Tweets (Rep. Dem.)",
"tweets_pop" = "Tweets (Populism)",
"news" = "News",
"news_short" = "News (Short)",
"manifestos" = "Manifestos",
"manifestos_multi" = "Manifestos Multi",
"stance" = "Stance",
"stance_long" = "Stance (Long)",
"mii" = "MII",
"mii_long" = "MII (Long)",
"synth" = "Synthetic",
"synth_short" = "Synthetic (Short)"
)
# Update the dataset column to be a factor with the desired order
ka_results <- ka_results %>%
mutate(dataset = factor(dataset, levels = facet_order))
# --- Step 7: Create the final styled plot ---
final_plot <- ggplot(ka_results, aes(x = temperature, y = ka_mean, color = frac, group =frac)) +
geom_line(alpha = 0.8) +
geom_point(alpha = 0.1) +
labs(
title = "",
x = "Temperature",
y = "inter-PSS",
color = "Dataset"
) +
ylim(0, 1) +
facet_wrap(
~ dataset,
scales = "free",
ncol = 4,
labeller = as_labeller(facet_labels)
) +
theme_minimal() +
theme(
legend.position = "bottom"
)
# Display the plot
print(final_plot)
# Save the plot to file
ggsave("plots/combined_between_subsamples.png", plot = final_plot, width = 6, height = 6, dpi = 300)
# Save the plot to file
ggsave("plots/combined_between_subsamples.png", plot = final_plot, width = 8, height = 6, dpi = 300)
# Save the plot to file
ggsave("plots/combined_between_subsamples.png", plot = final_plot, width = 8, height = 6, dpi = 300)
View(ka_results)
# --- Step 7: Create the final styled plot ---
final_plot <- ggplot(ka_results, aes(x = temperature, y = ka_mean, color = frac, group =frac)) +
geom_line(alpha = 0.8) +
geom_point(alpha = 0.1) +
labs(
title = "",
x = "Temperature",
y = "inter-PSS",
color = "%"
) +
ylim(0, 1) +
facet_wrap(
~ dataset,
scales = "free",
ncol = 4,
labeller = as_labeller(facet_labels)
) +
theme_minimal() +
theme(
legend.position = "bottom"
)
# Display the plot
print(final_plot)
# Save the plot to file
ggsave("plots/combined_between_subsamples.png", plot = final_plot, width = 8, height = 6, dpi = 300)
library(dplyr)
library(reticulate)
library(readr)
library(tibble)
library(ggplot2)
# Set up your Python environment
use_python("pssenv/bin/python", required = TRUE)
# Define output directory (adjust as needed)
output_dir <- "/Users/christopherbarrie/Dropbox/nyu_projects/promptstability/data/example"
# (Assuming you already created the intra cleaned CSV earlier.)
# For example, your intra file:
intra_file <- file.path(output_dir, "ollama_intra.csv")
# (If needed, you can run similar R code as for inter to create this file.)
# Read in the CSV files
df_intra <- read_csv(intra_file)
# Define a function to extract the integer following the final occurrence of "think>"
extract_integer_after_think <- function(annotation_text) {
extracted <- sub(".*think>\\s*(\\d+).*", "\\1", annotation_text)
return(as.numeric(extracted))
}
df_intra <- df_intra %>%
mutate(annotation_cleaned = extract_integer_after_think(annotation)) %>%
select(-ka_mean, -ka_lower, -ka_upper)
# Write the updated dataframes to new CSV files
intra_file <- file.path(output_dir, "ollama_intra_cleaned.csv")
write_csv(df_intra, intra_file)
# Build the Python script as a single pasted string.
python_script_intra <- paste0(
"import pandas as pd\n",
"from simpledorff import calculate_krippendorffs_alpha_for_df, metrics\n",
"\n",
"def calculate_intra_ka(file, dataset, annotator_col='iteration', class_col='annotation_cleaned'):\n",
"    df = pd.read_csv(file).copy()\n",
"    df[annotator_col] = pd.to_numeric(df[annotator_col], errors='coerce')\n",
"    df[class_col] = pd.to_numeric(df[class_col], errors='coerce')\n",
"    df = df.dropna(subset=[annotator_col, class_col])\n",
"    iterations = int(df[annotator_col].max()) + 1\n",
"    results = {}\n",
"    for i in range(1, iterations):\n",
"        subset = df[df[annotator_col] <= i]\n",
"        try:\n",
"            alpha = calculate_krippendorffs_alpha_for_df(\n",
"                subset,\n",
"                experiment_col='id',\n",
"                annotator_col=annotator_col,\n",
"                class_col=class_col,\n",
"                metric_fn=metrics.nominal_metric\n",
"            )\n",
"        except Exception as e:\n",
"            print(f'Error calculating KA for iteration {i}: {e}')\n",
"            alpha = None\n",
"        results[i] = alpha\n",
"        df.loc[df[annotator_col] == i, 'ka_mean'] = alpha\n",
"    return df, results\n",
"\n",
"intra_file = '", intra_file, "'\n",
"df_intra, ka_results = calculate_intra_ka(intra_file, dataset='manifestos')\n",
"output_intra_file = '", file.path(output_dir, "ka_results_intra.csv"), "'\n",
"df_intra.to_csv(output_intra_file, index=False)\n",
"print(f'Intra KA results written to {output_intra_file}')\n"
)
# Optionally, print the Python script to verify its contents
cat(python_script_intra)
# Execute the Python script via reticulate
reticulate::py_run_string(python_script_intra)
# ----- Read and plot the intra KA results in R -----
df_intra_ka <- read_csv(file.path(output_dir, "ka_results_intra.csv"))
# Optionally, add a model column (if you want to overlay with other models)
df_intra_ka <- df_intra_ka %>% mutate(model = "deepseek-r1-8b")
df1 <- read_csv("data/example/ka_results_intra.csv") %>%
distinct(iteration, ka_mean) %>%
mutate(model = "deepseek-r1-8b")
openai_file <- "data/example/openai_intra.csv"
df2 <- read_csv(openai_file) %>%
distinct(iteration, ka_mean) %>%
mutate(model = "gpt-4o")
df <- bind_rows(df1, df2)
# Plot KA by iteration
ggplot(df, aes(x = iteration, y = ka_mean, color = model)) +
geom_point(size = 3) +
geom_line() +
labs(
title = "Krippendorff's Alpha Across Iterations (Intra-Prompt)",
x = "Iteration",
y = "Krippendorff's Alpha",
color = "Model"
) +
theme_minimal()
# Plot KA by iteration
ggplot(df, aes(x = iteration, y = ka_mean, color = model)) +
geom_point(size = 3) +
geom_line() +
labs(
title = "Intra-PSS comparisons across models)",
x = "Iteration",
y = "intra-PSS",
color = "Model"
) +
ylim(0,1) +
theme_minimal()
